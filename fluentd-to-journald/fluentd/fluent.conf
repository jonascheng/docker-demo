<source>
  @type  forward
  @label @mainstream
  port  24224
</source>

@include routes/*.conf

<label @mainstream>
  <match nginx/**>
    @type relabel
    @label @nginx
  </match>

  <match zaplogger/**>
    @type relabel
    @label @zaplogger
  </match>

  <match randomlogger/**>
    @type relabel
    @label @randomlogger
  </match>

  # fall back no match
  <match **>
    @type stdout
  </match>
</label>

# route for serverity control
<label @severity_control>
  # insert tag into log record for later filter
  <filter **>
    @type record_transformer
    <record>
      tag ${tag}
    </record>
  </filter>

  # filter desired log according to tag field
  <filter **>
    @type grep
    <regexp>
      key tag
      pattern "#{ENV['SEVERITY']}"
    </regexp>
  </filter>

  # route to output
  <match **>
    @type relabel
    @label @output
  </match>
</label>

<label @output>
  # only keey log field
  <filter **>
    @type record_transformer
    renew_record true
    keep_keys log
  </filter>

  <match **>
    @type file
    path /fluentd/log/docker.*.log
    append true
    compress gzip
    time_slice_format %Y%m%d%H

    <buffer>
      @type file
      path /fluentd/log/docker.*.log
      # Chunks per hours ("3600" also available)
      timekey 1h
      # 1mins delay for flush ("300" also available)
      timekey_wait 1m
      # Flushes/writes chunks per specified time via flush_interval
      flush_mode interval
      flush_interval 30s
      flush_at_shutdown true
      # The maximum number of times to retry to flush the failed chunks
      retry_max_times 10
      # Control the buffer behavior when the queue becomes full – 3 modes supported – exception, block, drop oldest chunk
      overflow_action drop_oldest_chunk
      # If the bottom chunk fails to be written out, it will remain in the queue and Fluentd will retry after waiting retry_wait seconds
      retry_wait 1s
      # The maximum time (seconds) to retry to flush again the failed chunks, until the plugin discards the buffer chunks.
      # If the next retry is going to exceed this time limit, the last retry will be made at exactly this time limit.
      retry_timeout 1m
    </buffer>
  </match>
</label>

# fall back no match
<match **>
  @type stdout
</match>
